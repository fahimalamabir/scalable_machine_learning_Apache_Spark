{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Welcome to exercise one of “Apache Spark for Scalable Machine Learning on BigData”. In this exercise we’ll apply the basics of functional and parallel programming. \n",
    "\n",
    "Let’s start with a simple example. Let’s consider a list of integers.\n",
    "\n",
    "Let’s find out what the size of this list is."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "# <span style=\"color:red\"><<<<<!!!!! It seems that you are running in a IBM Watson Studio Apache Spark Notebook. Please run it in an IBM Watson Studio Default Runtime (without Apache Spark) !!!!!>>>>></span>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import Markdown, display\n",
    "def printmd(string):\n",
    "    display(Markdown('# <span style=\"color:red\">'+string+'</span>'))\n",
    "\n",
    "\n",
    "if ('sc' in locals() or 'sc' in globals()):\n",
    "    printmd('<<<<<!!!!! It seems that you are running in a IBM Watson Studio Apache Spark Notebook. Please run it in an IBM Watson Studio Default Runtime (without Apache Spark) !!!!!>>>>>')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    from pyspark import SparkContext, SparkConf\n",
    "    from pyspark.sql import SparkSession\n",
    "except ImportError as e:\n",
    "    printmd('<<<<<!!!!! Please restart your kernel after installing Apache Spark !!!!!>>>>>')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkContext\n",
    "sc = SparkContext.getOrCreate(SparkConf().setMaster(\"local[*]\"))\n",
    "\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd = sc.parallelize(range(100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should see \"100\" as answer. Now we want to know the sum of all elements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4950"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd.sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should get \"4950\" as answer.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let’s actually create a python function which decides whether a value is greater than 50 (True) or not (False).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gt50(i):\n",
    "    if i > 50:\n",
    "        return True\n",
    "    else:\n",
    "        return False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "print(gt50(4))\n",
    "print(gt50(51))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let’s simplify this function\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gt50(i):\n",
    "    return i > 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "print(gt50(4))\n",
    "print(gt50(51))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let’s use the lambda notation to define the function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "gt50 = lambda i: i > 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "print(gt50(4))\n",
    "print(gt50(51))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#let's shuffle our list to make it a bit more interesting\n",
    "from random import shuffle\n",
    "l = list(range(100))\n",
    "shuffle(l)\n",
    "rdd = sc.parallelize(l)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let’s filter values from our list which are equals or less than 50 by applying our “gt50” function to the list using the “filter” function. Note that by calling the “collect” function, all elements are returned to the Apache Spark Driver. This is not a good idea for BigData, please use “.sample(10,0.1).collect()” or “take(n)” instead.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[96,\n",
       " 86,\n",
       " 77,\n",
       " 71,\n",
       " 94,\n",
       " 81,\n",
       " 97,\n",
       " 68,\n",
       " 59,\n",
       " 89,\n",
       " 79,\n",
       " 88,\n",
       " 58,\n",
       " 92,\n",
       " 98,\n",
       " 54,\n",
       " 75,\n",
       " 51,\n",
       " 64,\n",
       " 67,\n",
       " 53,\n",
       " 69,\n",
       " 78,\n",
       " 83,\n",
       " 95,\n",
       " 66,\n",
       " 72,\n",
       " 85,\n",
       " 70,\n",
       " 56,\n",
       " 76,\n",
       " 52,\n",
       " 73,\n",
       " 80,\n",
       " 55,\n",
       " 74,\n",
       " 57,\n",
       " 60,\n",
       " 63,\n",
       " 82,\n",
       " 90,\n",
       " 84,\n",
       " 93,\n",
       " 62,\n",
       " 91,\n",
       " 99,\n",
       " 65,\n",
       " 87,\n",
       " 61]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd.filter(gt50).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also use the lambda function directly.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[96,\n",
       " 86,\n",
       " 77,\n",
       " 71,\n",
       " 94,\n",
       " 81,\n",
       " 97,\n",
       " 68,\n",
       " 59,\n",
       " 89,\n",
       " 79,\n",
       " 88,\n",
       " 58,\n",
       " 92,\n",
       " 98,\n",
       " 54,\n",
       " 75,\n",
       " 51,\n",
       " 64,\n",
       " 67,\n",
       " 53,\n",
       " 69,\n",
       " 78,\n",
       " 83,\n",
       " 95,\n",
       " 66,\n",
       " 72,\n",
       " 85,\n",
       " 70,\n",
       " 56,\n",
       " 76,\n",
       " 52,\n",
       " 73,\n",
       " 80,\n",
       " 55,\n",
       " 74,\n",
       " 57,\n",
       " 60,\n",
       " 63,\n",
       " 82,\n",
       " 90,\n",
       " 84,\n",
       " 93,\n",
       " 62,\n",
       " 91,\n",
       " 99,\n",
       " 65,\n",
       " 87,\n",
       " 61]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd.filter(lambda i: i > 50).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let’s consider the same list of integers. Now we want to compute the sum for elements in that list which are greater than 50 but less than 75. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1500"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd.filter(lambda x: x >50).filter(lambda x: x< 75).sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should see \"1500\" as answer. Now we want to know the sum of all elements. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this exercise you’ll create a DataFrame, register a temporary query table and issue SQL commands against it. \n",
    "\n",
    "Let’s create a little data frame:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+\n",
      "| id| value|\n",
      "+---+------+\n",
      "|  1|value1|\n",
      "|  2|value2|\n",
      "+---+------+\n",
      "\n",
      "root\n",
      " |-- id: long (nullable = true)\n",
      " |-- value: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import Row\n",
    "\n",
    "df = spark.createDataFrame([Row(id=1, value='value1'),Row(id=2, value='value2')])\n",
    "\n",
    "# let's have a look what's inside\n",
    "df.show()\n",
    "\n",
    "# let's print the schema\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we register this DataFrame as query table and issue an SQL statement against it. Please note that the result of the SQL execution returns a new DataFrame we can work with.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+\n",
      "| value|\n",
      "+------+\n",
      "|value2|\n",
      "+------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'value2'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# register dataframe as query table\n",
    "df.createOrReplaceTempView('df_view')\n",
    "\n",
    "# execute SQL query\n",
    "df_result = spark.sql('select value from df_view where id=2')\n",
    "\n",
    "# examine contents of result\n",
    "df_result.show()\n",
    "\n",
    "# get result as string\n",
    "df_result.first().value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
