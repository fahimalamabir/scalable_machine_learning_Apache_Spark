{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    from pyspark import SparkContext, SparkConf\n",
    "    from pyspark.sql import SparkSession\n",
    "except ImportError as e:\n",
    "    printmd('<<<<<!!!!! Please restart your kernel after installing Apache Spark !!!!!>>>>>')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc = SparkContext.getOrCreate(SparkConf().setMaster(\"local[*]\"))\n",
    "\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mean = $\\frac{1}{n} \\sum_{i=1}^n a_i$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "49.5\n"
     ]
    }
   ],
   "source": [
    "# create a rdd from 0 to 99\n",
    "rdd = sc.parallelize(range(100))\n",
    "sum_ = rdd.sum()\n",
    "n = rdd.count()\n",
    "mean = sum_/n\n",
    "print(mean)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Median <br>\n",
    "(1) sort the list <br>\n",
    "(2) pick the middle element <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0,\n",
       " 1,\n",
       " 2,\n",
       " 3,\n",
       " 4,\n",
       " 5,\n",
       " 6,\n",
       " 7,\n",
       " 8,\n",
       " 9,\n",
       " 10,\n",
       " 11,\n",
       " 12,\n",
       " 13,\n",
       " 14,\n",
       " 15,\n",
       " 16,\n",
       " 17,\n",
       " 18,\n",
       " 19,\n",
       " 20,\n",
       " 21,\n",
       " 22,\n",
       " 23,\n",
       " 24,\n",
       " 25,\n",
       " 26,\n",
       " 27,\n",
       " 28,\n",
       " 29,\n",
       " 30,\n",
       " 31,\n",
       " 32,\n",
       " 33,\n",
       " 34,\n",
       " 35,\n",
       " 36,\n",
       " 37,\n",
       " 38,\n",
       " 39,\n",
       " 40,\n",
       " 41,\n",
       " 42,\n",
       " 43,\n",
       " 44,\n",
       " 45,\n",
       " 46,\n",
       " 47,\n",
       " 48,\n",
       " 49,\n",
       " 50,\n",
       " 51,\n",
       " 52,\n",
       " 53,\n",
       " 54,\n",
       " 55,\n",
       " 56,\n",
       " 57,\n",
       " 58,\n",
       " 59,\n",
       " 60,\n",
       " 61,\n",
       " 62,\n",
       " 63,\n",
       " 64,\n",
       " 65,\n",
       " 66,\n",
       " 67,\n",
       " 68,\n",
       " 69,\n",
       " 70,\n",
       " 71,\n",
       " 72,\n",
       " 73,\n",
       " 74,\n",
       " 75,\n",
       " 76,\n",
       " 77,\n",
       " 78,\n",
       " 79,\n",
       " 80,\n",
       " 81,\n",
       " 82,\n",
       " 83,\n",
       " 84,\n",
       " 85,\n",
       " 86,\n",
       " 87,\n",
       " 88,\n",
       " 89,\n",
       " 90,\n",
       " 91,\n",
       " 92,\n",
       " 93,\n",
       " 94,\n",
       " 95,\n",
       " 96,\n",
       " 97,\n",
       " 98,\n",
       " 99]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0,\n",
       " 1,\n",
       " 2,\n",
       " 3,\n",
       " 4,\n",
       " 5,\n",
       " 6,\n",
       " 7,\n",
       " 8,\n",
       " 9,\n",
       " 10,\n",
       " 11,\n",
       " 12,\n",
       " 13,\n",
       " 14,\n",
       " 15,\n",
       " 16,\n",
       " 17,\n",
       " 18,\n",
       " 19,\n",
       " 20,\n",
       " 21,\n",
       " 22,\n",
       " 23,\n",
       " 24,\n",
       " 25,\n",
       " 26,\n",
       " 27,\n",
       " 28,\n",
       " 29,\n",
       " 30,\n",
       " 31,\n",
       " 32,\n",
       " 33,\n",
       " 34,\n",
       " 35,\n",
       " 36,\n",
       " 37,\n",
       " 38,\n",
       " 39,\n",
       " 40,\n",
       " 41,\n",
       " 42,\n",
       " 43,\n",
       " 44,\n",
       " 45,\n",
       " 46,\n",
       " 47,\n",
       " 48,\n",
       " 49,\n",
       " 50,\n",
       " 51,\n",
       " 52,\n",
       " 53,\n",
       " 54,\n",
       " 55,\n",
       " 56,\n",
       " 57,\n",
       " 58,\n",
       " 59,\n",
       " 60,\n",
       " 61,\n",
       " 62,\n",
       " 63,\n",
       " 64,\n",
       " 65,\n",
       " 66,\n",
       " 67,\n",
       " 68,\n",
       " 69,\n",
       " 70,\n",
       " 71,\n",
       " 72,\n",
       " 73,\n",
       " 74,\n",
       " 75,\n",
       " 76,\n",
       " 77,\n",
       " 78,\n",
       " 79,\n",
       " 80,\n",
       " 81,\n",
       " 82,\n",
       " 83,\n",
       " 84,\n",
       " 85,\n",
       " 86,\n",
       " 87,\n",
       " 88,\n",
       " 89,\n",
       " 90,\n",
       " 91,\n",
       " 92,\n",
       " 93,\n",
       " 94,\n",
       " 95,\n",
       " 96,\n",
       " 97,\n",
       " 98,\n",
       " 99]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd.sortBy(lambda x:x).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To access the middle element, we need to access the index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0, 0),\n",
       " (1, 1),\n",
       " (2, 2),\n",
       " (3, 3),\n",
       " (4, 4),\n",
       " (5, 5),\n",
       " (6, 6),\n",
       " (7, 7),\n",
       " (8, 8),\n",
       " (9, 9),\n",
       " (10, 10),\n",
       " (11, 11),\n",
       " (12, 12),\n",
       " (13, 13),\n",
       " (14, 14),\n",
       " (15, 15),\n",
       " (16, 16),\n",
       " (17, 17),\n",
       " (18, 18),\n",
       " (19, 19),\n",
       " (20, 20),\n",
       " (21, 21),\n",
       " (22, 22),\n",
       " (23, 23),\n",
       " (24, 24),\n",
       " (25, 25),\n",
       " (26, 26),\n",
       " (27, 27),\n",
       " (28, 28),\n",
       " (29, 29),\n",
       " (30, 30),\n",
       " (31, 31),\n",
       " (32, 32),\n",
       " (33, 33),\n",
       " (34, 34),\n",
       " (35, 35),\n",
       " (36, 36),\n",
       " (37, 37),\n",
       " (38, 38),\n",
       " (39, 39),\n",
       " (40, 40),\n",
       " (41, 41),\n",
       " (42, 42),\n",
       " (43, 43),\n",
       " (44, 44),\n",
       " (45, 45),\n",
       " (46, 46),\n",
       " (47, 47),\n",
       " (48, 48),\n",
       " (49, 49),\n",
       " (50, 50),\n",
       " (51, 51),\n",
       " (52, 52),\n",
       " (53, 53),\n",
       " (54, 54),\n",
       " (55, 55),\n",
       " (56, 56),\n",
       " (57, 57),\n",
       " (58, 58),\n",
       " (59, 59),\n",
       " (60, 60),\n",
       " (61, 61),\n",
       " (62, 62),\n",
       " (63, 63),\n",
       " (64, 64),\n",
       " (65, 65),\n",
       " (66, 66),\n",
       " (67, 67),\n",
       " (68, 68),\n",
       " (69, 69),\n",
       " (70, 70),\n",
       " (71, 71),\n",
       " (72, 72),\n",
       " (73, 73),\n",
       " (74, 74),\n",
       " (75, 75),\n",
       " (76, 76),\n",
       " (77, 77),\n",
       " (78, 78),\n",
       " (79, 79),\n",
       " (80, 80),\n",
       " (81, 81),\n",
       " (82, 82),\n",
       " (83, 83),\n",
       " (84, 84),\n",
       " (85, 85),\n",
       " (86, 86),\n",
       " (87, 87),\n",
       " (88, 88),\n",
       " (89, 89),\n",
       " (90, 90),\n",
       " (91, 91),\n",
       " (92, 92),\n",
       " (93, 93),\n",
       " (94, 94),\n",
       " (95, 95),\n",
       " (96, 96),\n",
       " (97, 97),\n",
       " (98, 98),\n",
       " (99, 99)]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd.sortBy(lambda x:x).zipWithIndex().collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "49.5\n"
     ]
    }
   ],
   "source": [
    "sortedandindexed = rdd.sortBy(lambda x:x).zipWithIndex().map(lambda x:x)\n",
    "n = sortedandindexed.count()\n",
    "if (n%2 == 1):\n",
    "    index = (n-1)/2;\n",
    "    print(sortedandindexed.lookup(index))\n",
    "else:\n",
    "    index1 = (n/2)-1\n",
    "    index2 = n/2\n",
    "    value1 = sortedandindexed.lookup(index1)[0]\n",
    "    value2 = sortedandindexed.lookup(index2)[0]\n",
    "    print((value1 + value2)/2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Standard Deviation:<br>\n",
    "    - tells you how wide the is spread around the mean<br>\n",
    "    so if SD is low, all the values should be close to the mean<br>\n",
    "    - to calculate it first calculate the mean $\\bar{x}$<br>\n",
    "    - SD = $\\sqrt{\\frac{1}{N}\\sum_{i=1}^N(x_i - \\bar{x})^2}$<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "28.86607004772212"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from math import sqrt\n",
    "sum_ = rdd.sum()\n",
    "n = rdd.count()\n",
    "mean = sum_/n\n",
    "sqrt(rdd.map(lambda x: pow(x-mean,2)).sum()/n)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Skewness<br>\n",
    "- tells us how asymmetric data is spread around the mean <br>\n",
    "- check positive skew, negative skew <br>\n",
    "- Skew = $\\frac{1}{n}\\frac{\\sum_{j=1}^n (x_j- \\bar{x})^3}{\\text{SD}^3}$, x_j= individual value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.4210854715202004e-16"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sd= sqrt(rdd.map(lambda x: pow(x-mean,2)).sum()/n)\n",
    "n = float(n) # to round off\n",
    "skw = (1/n)*rdd.map(lambda x : pow(x- mean,3)/pow(sd,3)).sum()\n",
    "skw"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Kurtosis<br>\n",
    "\n",
    "- tells us the shape of the data<br>\n",
    "- indicates outlier content within the data<br>\n",
    "- kurt = $\\frac{1}{n}\\frac{\\sum_{j=1}^n (x_j- \\bar{x})^4}{\\text{SD}^4}$, x_j= individual value\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.7997599759975997"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(1/n)*rdd.map(lambda x : pow(x- mean,4)/pow(sd,4)).sum()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Covariance \\& Correlation<br>\n",
    "\n",
    "- how two columns interact with each other<br>\n",
    "- how all columns interact with each other<br>\n",
    "- cov(X,Y) = $\\frac{1}{n} \\sum_{i=1}^n (x_i-\\bar{x})(y_i -\\bar{y})$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "rddX = sc.parallelize(range(100))\n",
    "rddY = sc.parallelize(range(100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to avoid loss of precision use float\n",
    "meanX = rddX.sum()/float(rddX.count())\n",
    "meanY = rddY.sum()/float(rddY.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "833.25"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# since we need to use rddx, rddy same time we need to zip them together\n",
    "rddXY = rddX.zip(rddY)\n",
    "covXY = rddXY.map(lambda x:(x[0]-meanX)*(x[1]-meanY)).sum()/rddXY.count()\n",
    "covXY"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Correlation\n",
    "\n",
    "- corr(X,Y) =$ \\frac{\\text{cov(X,Y)}}{SD_X SD_Y}$\n",
    "<br>\n",
    "\n",
    "Measure of dependency - Correlation<br>\n",
    " +1 Columns totally correlate<br>\n",
    " 0 columns show no interaction<br>\n",
    " -1 inverse dependency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from math import sqrt\n",
    "n = rddXY.count()\n",
    "mean = sum_/n\n",
    "SDX = sqrt(rdd.map(lambda x: pow(x-meanX,2)).sum()/n)\n",
    "SDY = sqrt(rdd.map(lambda y: pow(y-meanY,2)).sum()/n)\n",
    "corrXY = covXY/(SDX *SDY)\n",
    "corrXY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 1, 2, 3, 4]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# corellation matrix in practice\n",
    "import random\n",
    "from pyspark.mllib.stat import Statistics\n",
    "col1 = sc.parallelize(range(100))\n",
    "col2 = sc.parallelize(range(100,200))\n",
    "col3 = sc.parallelize(list(reversed(range(100))))\n",
    "col4 = sc.parallelize(random.sample(range(100),100))\n",
    "data = col1\n",
    "data.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0, 100), (1, 101), (2, 102), (3, 103), (4, 104)]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data1 = col1.zip(col2)\n",
    "data1.take(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Welcome to exercise one of week two of “Apache Spark for Scalable Machine Learning on BigData”. In this exercise you’ll read a DataFrame in order to perform a simple statistical analysis. Then you’ll rebalance the dataset. No worries, we’ll explain everything to you, let’s get started.\n",
    "\n",
    "Let’s create a data frame from a remote file by downloading it:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2020-11-06 02:38:52--  https://github.com/IBM/coursera/raw/master/hmp.parquet\n",
      "Resolving github.com (github.com)... 140.82.114.3\n",
      "Connecting to github.com (github.com)|140.82.114.3|:443... connected.\n",
      "HTTP request sent, awaiting response... 301 Moved Permanently\n",
      "Location: https://github.com/IBM/skillsnetwork/raw/master/hmp.parquet [following]\n",
      "--2020-11-06 02:38:52--  https://github.com/IBM/skillsnetwork/raw/master/hmp.parquet\n",
      "Reusing existing connection to github.com:443.\n",
      "HTTP request sent, awaiting response... 302 Found\n",
      "Location: https://raw.githubusercontent.com/IBM/skillsnetwork/master/hmp.parquet [following]\n",
      "--2020-11-06 02:38:53--  https://raw.githubusercontent.com/IBM/skillsnetwork/master/hmp.parquet\n",
      "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 151.101.52.133\n",
      "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|151.101.52.133|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 932997 (911K) [application/octet-stream]\n",
      "Saving to: ‘hmp.parquet’\n",
      "\n",
      "hmp.parquet         100%[===================>] 911.13K  3.79MB/s    in 0.2s    \n",
      "\n",
      "2020-11-06 02:38:53 (3.79 MB/s) - ‘hmp.parquet’ saved [932997/932997]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# delete files from previous runs\n",
    "!rm -f hmp.parquet*\n",
    "\n",
    "# download the file containing the data in PARQUET format\n",
    "!wget https://github.com/IBM/coursera/raw/master/hmp.parquet\n",
    "    \n",
    "# create a dataframe out of it\n",
    "df = spark.read.parquet('hmp.parquet')\n",
    "\n",
    "# register a corresponding query table\n",
    "df.createOrReplaceTempView('df')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a classical classification data set. One thing we always do during data analysis is checking if the classes are balanced. In other words, if there are more or less the same number of example in each class. Let’s find out by a simple aggregation using SQL.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[class: string, count: bigint]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from pyspark.sql.functions import col\n",
    "counts = df.groupBy('class').count().orderBy('count')\n",
    "display(counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+-----+\n",
      "|         class|count|\n",
      "+--------------+-----+\n",
      "| Use_telephone|15225|\n",
      "| Standup_chair|25417|\n",
      "|      Eat_meat|31236|\n",
      "|     Getup_bed|45801|\n",
      "|   Drink_glass|42792|\n",
      "|    Pour_water|41673|\n",
      "|     Comb_hair|23504|\n",
      "|          Walk|92254|\n",
      "|  Climb_stairs|40258|\n",
      "| Sitdown_chair|25036|\n",
      "|   Liedown_bed|11446|\n",
      "|Descend_stairs|15375|\n",
      "|   Brush_teeth|29829|\n",
      "|      Eat_soup| 6683|\n",
      "+--------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.groupBy('class').count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+--------+\n",
      "|         class|count(1)|\n",
      "+--------------+--------+\n",
      "| Use_telephone|   15225|\n",
      "| Standup_chair|   25417|\n",
      "|      Eat_meat|   31236|\n",
      "|     Getup_bed|   45801|\n",
      "|   Drink_glass|   42792|\n",
      "|    Pour_water|   41673|\n",
      "|     Comb_hair|   23504|\n",
      "|          Walk|   92254|\n",
      "|  Climb_stairs|   40258|\n",
      "| Sitdown_chair|   25036|\n",
      "|   Liedown_bed|   11446|\n",
      "|Descend_stairs|   15375|\n",
      "|   Brush_teeth|   29829|\n",
      "|      Eat_soup|    6683|\n",
      "+--------------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql('select class,count(*) from df group by class').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This looks nice, but it would be nice if we can aggregate further to obtain some quantitative metrics on the imbalance like, min, max, mean and standard deviation. If we divide max by min we get a measure called minmax ration which tells us something about the relationship between the smallest and largest class. Again, let’s first use SQL for those of you familiar with SQL. Don’t be scared, we’re used nested sub-selects, basically selecting from a result of a SQL query like it was a table. All within on SQL statement.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+------------------+------------------+-----------------+\n",
      "| min|  max|              mean|            stddev|      minmaxratio|\n",
      "+----+-----+------------------+------------------+-----------------+\n",
      "|6683|92254|31894.928571428572|21284.893716741157|13.80427951518779|\n",
      "+----+-----+------------------+------------------+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql('''\n",
    "    select \n",
    "        *,\n",
    "        max/min as minmaxratio -- compute minmaxratio based on previously computed values\n",
    "        from (\n",
    "            select \n",
    "                min(ct) as min, -- compute minimum value of all classes\n",
    "                max(ct) as max, -- compute maximum value of all classes\n",
    "                mean(ct) as mean, -- compute mean between all classes\n",
    "                stddev(ct) as stddev -- compute standard deviation between all classes\n",
    "                from (\n",
    "                    select\n",
    "                        count(*) as ct -- count the number of rows per class and rename it to ct\n",
    "                        from df -- access the temporary query table called df backed by DataFrame df\n",
    "                        group by class -- aggrecate over class\n",
    "                )\n",
    "        )   \n",
    "''').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The same query can be expressed using the DataFrame API. Again, don’t be scared. It’s just a sequential expression of transformation steps. You now an choose which syntax you like better.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+---+--------------------+-----------+\n",
      "|  x|  y|  z|              source|      class|\n",
      "+---+---+---+--------------------+-----------+\n",
      "| 22| 49| 35|Accelerometer-201...|Brush_teeth|\n",
      "| 22| 49| 35|Accelerometer-201...|Brush_teeth|\n",
      "| 22| 52| 35|Accelerometer-201...|Brush_teeth|\n",
      "| 22| 52| 35|Accelerometer-201...|Brush_teeth|\n",
      "| 21| 52| 34|Accelerometer-201...|Brush_teeth|\n",
      "| 22| 51| 34|Accelerometer-201...|Brush_teeth|\n",
      "| 20| 50| 35|Accelerometer-201...|Brush_teeth|\n",
      "| 22| 52| 34|Accelerometer-201...|Brush_teeth|\n",
      "| 22| 50| 34|Accelerometer-201...|Brush_teeth|\n",
      "| 22| 51| 35|Accelerometer-201...|Brush_teeth|\n",
      "| 21| 51| 33|Accelerometer-201...|Brush_teeth|\n",
      "| 20| 50| 34|Accelerometer-201...|Brush_teeth|\n",
      "| 21| 49| 33|Accelerometer-201...|Brush_teeth|\n",
      "| 21| 49| 33|Accelerometer-201...|Brush_teeth|\n",
      "| 20| 51| 35|Accelerometer-201...|Brush_teeth|\n",
      "| 18| 49| 34|Accelerometer-201...|Brush_teeth|\n",
      "| 19| 48| 34|Accelerometer-201...|Brush_teeth|\n",
      "| 16| 53| 34|Accelerometer-201...|Brush_teeth|\n",
      "| 18| 52| 35|Accelerometer-201...|Brush_teeth|\n",
      "| 18| 51| 32|Accelerometer-201...|Brush_teeth|\n",
      "+---+---+---+--------------------+-----------+\n",
      "only showing top 20 rows\n",
      "\n",
      "root\n",
      " |-- x: integer (nullable = true)\n",
      " |-- y: integer (nullable = true)\n",
      " |-- z: integer (nullable = true)\n",
      " |-- source: string (nullable = true)\n",
      " |-- class: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+------------------+------------------+-----------------+\n",
      "| min|  max|              mean|            stddev|      minmaxratio|\n",
      "+----+-----+------------------+------------------+-----------------+\n",
      "|6683|92254|31894.928571428572|21284.893716741157|13.80427951518779|\n",
      "+----+-----+------------------+------------------+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col, min, max, mean, stddev\n",
    "\n",
    "df \\\n",
    "    .groupBy('class') \\\n",
    "    .count() \\\n",
    "    .select([ \n",
    "        min(col(\"count\")).alias('min'), \n",
    "        max(col(\"count\")).alias('max'), \n",
    "        mean(col(\"count\")).alias('mean'), \n",
    "        stddev(col(\"count\")).alias('stddev') \n",
    "    ]) \\\n",
    "    .select([\n",
    "        col('*'),\n",
    "        (col(\"max\") / col(\"min\")).alias('minmaxratio')\n",
    "    ]) \\\n",
    "    .show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now it’s time for you to work on the data set. First, please create a table of all classes with the respective counts, but this time, please order the table by the count number, ascending.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+-----+\n",
      "|         class|count|\n",
      "+--------------+-----+\n",
      "|      Eat_soup| 6683|\n",
      "|   Liedown_bed|11446|\n",
      "| Use_telephone|15225|\n",
      "|Descend_stairs|15375|\n",
      "|     Comb_hair|23504|\n",
      "| Sitdown_chair|25036|\n",
      "| Standup_chair|25417|\n",
      "|   Brush_teeth|29829|\n",
      "|      Eat_meat|31236|\n",
      "|  Climb_stairs|40258|\n",
      "|    Pour_water|41673|\n",
      "|   Drink_glass|42792|\n",
      "|     Getup_bed|45801|\n",
      "|          Walk|92254|\n",
      "+--------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df1 = df.groupBy('class').count()\n",
    "df1.sort('count',ascending=True).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pixiedust is a very sophisticated library. It takes care of sorting as well. Please modify the bar chart so that it gets sorted by the number of elements per class, ascending. Hint: It’s an option available in the UI once rendered using the display() function.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "pixiedust": {
     "displayParams": {}
    }
   },
   "outputs": [],
   "source": [
    "import pixiedust\n",
    "from pyspark.sql.functions import col\n",
    "counts = df.groupBy('class').count().orderBy('count')\n",
    "display(counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Imbalanced classes can cause pain in machine learning. Therefore let’s rebalance. In the flowing we limit the number of elements per class to the amount of the least represented class. This is called undersampling. Other ways of rebalancing can be found here:\n",
    "\n",
    "[https://machinelearningmastery.com/tactics-to-combat-imbalanced-classes-in-your-machine-learning-dataset/](https://machinelearningmastery.com/tactics-to-combat-imbalanced-classes-in-your-machine-learning-dataset?cm_mmc=Email_Newsletter-_-Developer_Ed%2BTech-_-WW_WW-_-SkillsNetwork-Courses-IBMDeveloperSkillsNetwork-ML0201EN-SkillsNetwork-20647446&cm_mmca1=000026UJ&cm_mmca2=10006555&cm_mmca3=M12345678&cvosrc=email.Newsletter.M12345678&cvo_campaign=000026UJ)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import min\n",
    "\n",
    "# create a lot of distinct classes from the dataset\n",
    "classes = [row[0] for row in df.select('class').distinct().collect()]\n",
    "\n",
    "# compute the number of elements of the smallest class in order to limit the number of samples per calss\n",
    "min = df.groupBy('class').count().select(min('count')).first()[0]\n",
    "\n",
    "# define the result dataframe variable\n",
    "df_balanced = None\n",
    "\n",
    "# iterate over distinct classes\n",
    "for cls in classes:\n",
    "    \n",
    "    # only select examples for the specific class within this iteration\n",
    "    # shuffle the order of the elements (by setting fraction to 1.0 sample works like shuffle)\n",
    "    # return only the first n samples\n",
    "    df_temp = df \\\n",
    "        .filter(\"class = '\"+cls+\"'\") \\\n",
    "        .sample(False, 1.0) \\\n",
    "        .limit(min)\n",
    "    \n",
    "    # on first iteration, assing df_temp to empty df_balanced\n",
    "    if df_balanced == None:    \n",
    "        df_balanced = df_temp\n",
    "    # afterwards, append vertically\n",
    "    else:\n",
    "        df_balanced=df_balanced.union(df_temp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please verify, by using the code cell below, if df_balanced has the same number of elements per class. You should get 6683 elements per class.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-27-0b6fdb7ac0aa>, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-27-0b6fdb7ac0aa>\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    $$$\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "$$$"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
